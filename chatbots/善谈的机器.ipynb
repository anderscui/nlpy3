{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packets of thought (NLP概览）\n",
    "\n",
    "## 1.4 计算机所能理解的“语言”\n",
    "\n",
    "最简单的方式是创建一系列的规则，使用各种`if ... else ...`表示。这等价于编写一种特殊的程序，即**优先状态机**（FSM，finite state machine）。如果你使用过正则表达式，那么你已经使用过FSM了。FSM是NLP中的方法之一。\n",
    "\n",
    "纯粹根据语句的形式（基于一组规则）来判断，我们要用到**形式语言(formal language)**。形式语言是自然语言的子集。\n",
    "\n",
    "形式语言与自然语言的区别：\n",
    "编程语言的主要特点：\n",
    "* 上下文无关语言\n",
    "* 使用上下文无关文法解析，解析颇为高效\n",
    "* 正则语言也可以高效解析\n",
    "自然语言：\n",
    "* 不是正则的\n",
    "* 不是上下文无关的\n",
    "* 不能使用形式语法来定义\n",
    "\n",
    "### 1.4.3 一个简单的chatbot\n",
    "\n",
    "该bot基于模式匹配，此方法在现代的ML方法之前是很常见的，包括Amazon Alexa之类的chatbot亦使用此方法。\n",
    "\n",
    "可以看到，尽管这里脑补了较为复杂的模式，还是很难避免出现两类问题：false negative和false positive。一方面，可说它太严格了，另一方面，也可说它太宽松了。\n",
    "\n",
    "由于算力的缺乏，早期NLP研究者必须使用人脑来弥补，添加很多复杂的人工规则。这类方法被称为pattern-base approach，它们不仅可以匹配字符序列，还可以匹配更高级别的模式，如词序列、词性等。stemmer、tokenizer及ELIZA这样的对话引擎都是基于这类方法的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = r\"[^a-z]*([y]o|[h']?ello|ok|hey|(good[ ])?(morn[gin']{0,3}|\"\\\n",
    "    r\"afternoon|even[gin']{0,3}))[\\s,;:]{1,3}([a-z]{1,20})\"\n",
    "re_greeting = re.compile(r, flags=re.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(0, 10), match='Hello Rosa'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_greeting.match('Hello Rosa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(0, 12), match='Morning Rosa'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_greeting.match('Morning Rosa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出something\n",
    "\n",
    "my_names = {'rosa', 'rose', 'chatbot', 'bot'}\n",
    "curt_names = {'hal', 'you', 'u'}\n",
    "greeter_name = 'NLP'\n",
    "\n",
    "\n",
    "def generate(s):\n",
    "    m = re_greeting.match(s)\n",
    "    if m:\n",
    "        at_name = m.groups()[-1]\n",
    "        if at_name in curt_names:\n",
    "            print('good one')\n",
    "        elif at_name.lower() in my_names:\n",
    "            print(f'hi {greeter_name}, how are you?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi NLP, how are you?\n"
     ]
    }
   ],
   "source": [
    "generate('hello Rosa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.4 另一种方法\n",
    "\n",
    "除了基于规则的方法，如果我们有足够多的数据，是否可以基于统计来完成对话系统呢？最直接的想法是，看一下当前问题在数据库中有没有，如果有，那么从已有的回答中，返回一条。这种方法的问题是，一个问句，要么“在”要么“不在”数据库中，它很容易受拼写错误和同义词之类的影响。我们需要一个合理的方法，来度量两个句子**语义**的“相似度”。\n",
    "\n",
    "单纯从字面的字符来判断语义相似度很容易出问题，即使借Jaccard、Levenshtein、Euclidean向量距离之类的方法，也难以有大的改观。不过如基于规则的方法一样，此类方法也有它适合的领域，如拼写检查、专有名词提取等。如果我们关注语义胜于拼写，那么需要考虑其它更好的方法。\n",
    "\n",
    "## 1.5 hyperspace 一瞥\n",
    "\n",
    "第三章中将涉及**cosine distance**，第四章将借助它来完成向量的降维。\n",
    "\n",
    "## 1.6 词序与语法\n",
    "\n",
    "词序很重要，语言中与词序有关的规则称为“语法”。如果使用词袋模型，那么词序信息就丢失了。在实现自然语言查询之类的应用时，词袋不是一个很好的选择，幸运的是，目前的主流工具在语法树解析上有很高的准确率，如Spacy有93%，SyntaxNet有94%。\n",
    "\n",
    "## 1.7 chatbot NLP pipeline\n",
    "\n",
    "chatbot的pipeline需要四个处理阶段（stage），也需要有数据库维护过去的对话与响应。每个阶段都可能包含一个或多个处理算法，以并行或串行的方式执行。\n",
    "\n",
    "* Parse: 从自然语言文本中提取特征、结构化数据\n",
    "* Analyze：通过情感、语法和语义来泛化、合并特征\n",
    "* Generate：通过模板、搜索和语言模型生成响应信息\n",
    "* Execute：基于对话历史与目标计划语句，并选择下一个响应\n",
    "\n",
    "深度学习和数据驱动编程使得NLP和chatbot的应用愈发广泛。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2、构造词汇表\n",
    "\n",
    "* tokenizing text\n",
    "* 处理不标准的表达与表情符号，如来自社交媒体的那些\n",
    "* stemming、lemmatization\n",
    "* 构造语句的向量表示\n",
    "* 构造sentiment analyzer\n",
    "\n",
    "本章主要讨论如何将文档（任意字符串）分解为离散的、具有一定意义的token。\n",
    "\n",
    "基本的token近似于*词（word）*，但一般来说，token不一定是词。比如”ice cream“，我们更愿意将其作为一个整体来看待。另一方面，词甚至还能进一步分解为音节（syllable）、前缀、后缀，乃至字母（letter/grapheme）。\n",
    "\n",
    "词可能是”隐含的“或”不可见的“，不如当你说出”Don't“时。\n",
    "\n",
    "与词相关的概念是*n-grams*。为完整起见，这里的方法将保留所有的词以及n-grams，但一般来说，特征提取很少会保留所有信息，至于保留多少这就是NLP中的艺术了。\n",
    "\n",
    "## 2.1 stemming的挑战\n",
    "\n",
    "stemming是指把同一个词的不同屈折形式归为一类，某些人花费了毕生精力来完成此事。考虑一个例子，后缀”ing“对于以下几个词：ending、running、sing，或复数形式的”s“对于以下几个词：words、bus、lens。这一问题的解决办法，一是传统的基于规则，二是基于统计方法，有了足够多的数据，可以完全避免人工规则。\n",
    "\n",
    "## 2.2 通过tokenizer构建词汇表\n",
    "\n",
    "在NLP中，标记化（tokenization）是一种特殊的文档分割（segmentation）过程。segmentation将文本分割为更小的部分，如段落、句子、短语或token（通常为词）。顾名思义，tokenization就是将文本分割为token。\n",
    "\n",
    "作为NLP的基础组块，与编译器中的某些部分可视为等价物：\n",
    "\n",
    "* tokenizer：scanner、lexer\n",
    "* vocabulary：lexicon\n",
    "* parser：compiler\n",
    "* token、term、word、n-gram：token、symbol\n",
    "\n",
    "tokenization是NLP pipeline的第一步，对后续步骤可能会产生巨大影响。tokenizer将非结构化的数据转化为离散的信息块，这些块的**统计信息**本身即形成一个向量，从而可用于某些ML算法。BOW向量适合于搜索之类的应用。\n",
    "\n",
    "最简单的方法是使用空白字符分割：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thomas',\n",
       " 'Jefferson',\n",
       " 'began',\n",
       " 'building',\n",
       " 'Monticello',\n",
       " 'at',\n",
       " 'the',\n",
       " 'age',\n",
       " 'of',\n",
       " '26.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"Thomas Jefferson began building Monticello at the age of 26.\"\n",
    "sent.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上例中，除了末尾的句号，其它部分是没问题的。先不管它，现在考虑至此的第一种向量表示法：**one-hot vectors**，它可以将文本表示为”数值“："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab: 26. Jefferson Monticello Thomas age at began building of the\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "tokens = sent.split()\n",
    "vocab = sorted(set(tokens))\n",
    "print('vocab:', ' '.join(vocab))\n",
    "\n",
    "n_tokens = len(tokens)\n",
    "n_vocab = len(vocab)\n",
    "\n",
    "one_hot_vecs = np.zeros((n_tokens, n_vocab), int)\n",
    "for i, w in enumerate(tokens):\n",
    "    one_hot_vecs[i, vocab.index(w)] = 1\n",
    "    \n",
    "one_hot_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的关键信息是，矩阵的维度是(n_tokens, n_vocab)，即行的顺序与tokens顺序一致，每一行表示相应的token信息，在一行上值为1的那个元素表示token是哪一个。通过这个矩阵及`vocab`表，确实可以保存下句子的完整信息。这个矩阵亦可以通过`DataFrame`表示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>26.</th>\n",
       "      <th>Jefferson</th>\n",
       "      <th>Monticello</th>\n",
       "      <th>Thomas</th>\n",
       "      <th>age</th>\n",
       "      <th>at</th>\n",
       "      <th>began</th>\n",
       "      <th>building</th>\n",
       "      <th>of</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   26.  Jefferson  Monticello  Thomas  age  at  began  building  of  the\n",
       "0    0          0           0       1    0   0      0         0   0    0\n",
       "1    0          1           0       0    0   0      0         0   0    0\n",
       "2    0          0           0       0    0   0      1         0   0    0\n",
       "3    0          0           0       0    0   0      0         1   0    0\n",
       "4    0          0           1       0    0   0      0         0   0    0\n",
       "5    0          0           0       0    0   1      0         0   0    0\n",
       "6    0          0           0       0    0   0      0         0   0    1\n",
       "7    0          0           0       0    1   0      0         0   0    0\n",
       "8    0          0           0       0    0   0      0         0   1    0\n",
       "9    1          0           0       0    0   0      0         0   0    0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(one_hot_vecs, columns=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hot的意思是”on“，每一行只有一个元素为1，所以是one-hot。该方法得到的数据是无损的，通常可用于NN、seq-to-seq语言模型、生成式语言模型。但是这个表无疑是太大了，尤其是在列上，词汇表可能会扩大到数以百万计。这样，即使只是保存几千本书，可能就需要数百T的容量。\n",
    "\n",
    "如果放弃”完整信息“，比如讲所有行相加，那么就得到了BOW向量，或词频向量，它仅关心词的发生频率，忽略了顺序。\n",
    "\n",
    "如果需要实现关键词搜索，只要对两个向量做OR运算。下面是bow的简单实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('26.', 1),\n",
       " ('Jefferson', 1),\n",
       " ('Monticello', 1),\n",
       " ('Thomas', 1),\n",
       " ('age', 1),\n",
       " ('at', 1),\n",
       " ('began', 1),\n",
       " ('building', 1),\n",
       " ('of', 1),\n",
       " ('the', 1)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_bow = {token: 1 for token in tokens}\n",
    "sorted(sent_bow.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词袋模型，除了只用一个向量即可表示文档，还有一个极大的好处时，它只保留出现的词，不用考虑整个词汇表。这个dict还可以进一步压缩，比如将每个词表示一个整型值（它的索引），Spacy就是这么做的。如上面用`DataFrame`类似，词袋可以表示为`Series`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Thomas</th>\n",
       "      <th>Jefferson</th>\n",
       "      <th>began</th>\n",
       "      <th>building</th>\n",
       "      <th>Monticello</th>\n",
       "      <th>at</th>\n",
       "      <th>the</th>\n",
       "      <th>age</th>\n",
       "      <th>of</th>\n",
       "      <th>26.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sent</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Thomas  Jefferson  began  building  Monticello  at  the  age  of  26.\n",
       "sent       1          1      1         1           1   1    1    1   1    1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(pd.Series(sent_bow), columns=['sent']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1770.</th>\n",
       "      <th>26.</th>\n",
       "      <th>Construction</th>\n",
       "      <th>He</th>\n",
       "      <th>Jefferson</th>\n",
       "      <th>Jefferson's</th>\n",
       "      <th>Monticello</th>\n",
       "      <th>Pavilion</th>\n",
       "      <th>South</th>\n",
       "      <th>Thomas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sent0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       1770.  26.  Construction  He  Jefferson  Jefferson's  Monticello  \\\n",
       "sent0      0    1             0   0          1            0           1   \n",
       "sent1      0    0             1   0          0            0           0   \n",
       "sent2      1    0             0   1          0            0           0   \n",
       "sent3      0    0             0   0          0            1           1   \n",
       "\n",
       "       Pavilion  South  Thomas  \n",
       "sent0         0      0       1  \n",
       "sent1         0      0       0  \n",
       "sent2         1      1       0  \n",
       "sent3         0      0       0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 现在可以使用多一点文档了\n",
    "sents = ['Thomas Jefferson began building Monticello at the age of 26.',\n",
    "         'Construction was done mostly by local masons and carpenters.',\n",
    "         'He moved into the South Pavilion in 1770.',\n",
    "         \"Turning Monticello into a neoclassical masterpiece was Jefferson's obsession.\"]\n",
    "\n",
    "corpus = {}\n",
    "\n",
    "for i, sent in enumerate(sents):\n",
    "    corpus[f'sent{i}'] = {token: 1 for token in sent.split()}\n",
    "df = pd.DataFrame.from_records(corpus).fillna(0).astype(int).T\n",
    "df[df.columns[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在有了多个文档的向量表示，那么有时需要比较它们的相似度，最基本的方法会用到**点积（dot product）**。\n",
    "\n",
    "### 2.2.2 BOW的相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df.T\n",
    "df2.sent0.dot(df2.sent1), df2.sent0.dot(df2.sent3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里点积的值表明两个文档间有多少个共同的token。BOW是接触到的第一种向量空间模型（VSM），除了点积，它还可以进行加法、减法、OR、AND等操作，后者对于搜索之类的应用有主要的意义。\n",
    "\n",
    "### 2.2.3 更好的token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thomas',\n",
       " 'Jefferson',\n",
       " 'began',\n",
       " 'building',\n",
       " 'Monticello',\n",
       " 'at',\n",
       " 'the',\n",
       " 'age',\n",
       " 'of',\n",
       " '26']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "spliter = re.compile(r'[-\\s.,;!?]+', re.U)\n",
    "\n",
    "sent = 'Thomas Jefferson began building Monticello at the age of 26.'\n",
    "tokens = spliter.split(sent)\n",
    "tokens = [t for t in tokens if t and t not in '- \\t\\n.,;!?']\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于英语，几个常见的库都包含了tokenizer，如`spaCy`、`Stanford CoreNLP`、`NLTK`，后两个主要用于学术领域，spaCy则是工业级的。\n",
    "\n",
    "值得一提的是，NLTK还包含了一个`casual_tokenize`，可用于Twitter、Facebook等社交网站的文本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thomas',\n",
       " 'Jefferson',\n",
       " 'began',\n",
       " 'building',\n",
       " 'Monticello',\n",
       " 'at',\n",
       " 'the',\n",
       " 'age',\n",
       " 'of',\n",
       " '26',\n",
       " '.']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+|$[0-9.]+|\\S+')\n",
    "\n",
    "sent = 'Thomas Jefferson began building Monticello at the age of 26.'\n",
    "tokenizer.tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Monticello',\n",
       " 'was',\n",
       " \"n't\",\n",
       " 'designated',\n",
       " 'as',\n",
       " 'UNESCO',\n",
       " 'World',\n",
       " 'Heritage',\n",
       " 'Site',\n",
       " 'until',\n",
       " '1987',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "sent = \"Monticello wasn't designated as UNESCO World Heritage Site until 1987.\"\n",
    "\n",
    "# treebank tokenizer可以处理contraction的情形\n",
    "tokenizer.tokenize(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 通过n-grams扩展词汇\n",
    "\n",
    "仅通过token（unigram），无法判断一个同时包含`ice`和`cream`的文档是否真地与`ice cream`有关，换言之，它丢失了所有与词序有关的信息，而n-grams保留了少许。通常，我们可以说，n-grams保留了一定的**上下文**信息。\n",
    "\n",
    "需要考虑的是，n-grams不一定是”有效的“复合词或词组，它仅仅表明某些词共同出现（共现）了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thomas',\n",
       " 'Jefferson',\n",
       " 'began',\n",
       " 'building',\n",
       " 'Monticello',\n",
       " 'at',\n",
       " 'the',\n",
       " 'age',\n",
       " 'of',\n",
       " '26']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spliter = re.compile(r'[-\\s.,;!?]+', re.U)\n",
    "\n",
    "sent = 'Thomas Jefferson began building Monticello at the age of 26.'\n",
    "tokens = spliter.split(sent)\n",
    "tokens = [t for t in tokens if t and t not in '- \\t\\n.,;!?']\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Thomas', 'Jefferson'),\n",
       " ('Jefferson', 'began'),\n",
       " ('began', 'building'),\n",
       " ('building', 'Monticello'),\n",
       " ('Monticello', 'at'),\n",
       " ('at', 'the'),\n",
       " ('the', 'age'),\n",
       " ('age', 'of'),\n",
       " ('of', '26')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "list(ngrams(tokens, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般来说，如果token或n-grams出现频率极低，那么可认为它未携带足够的相关信息，这里的相关是指与文档的主题或分类有关。同时，token的组合数要比token数多很多，而如果特征向量的维度超过了文档数，那么特征提取这一步骤会给后续步骤带来不利影响，出现过拟合的情况。所以频率过低的n-grams一般会过滤掉。\n",
    "\n",
    "另一方面，如果频率过高，我们也可以认为它是无区分度的，因此也会被忽略掉。这种词或n-grams通常称为**Stop Words**。\n",
    "\n",
    "不过，在使用n-grams时，停用词不能够简单地去掉，否则会影响真正的n-grams，而且此时，去掉少数n-grams给内存等带来的收益不大。此时还是保留为好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/andersc/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "318"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as sklearn_stopwords\n",
    "len(sklearn_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.5 normalize词汇表\n",
    "\n",
    "减小词汇表大小可以减少过拟合的可能性。\n",
    "\n",
    "* case folding: 是否统一大小写，取决于当前应用（如NER等）\n",
    "* stemming：对搜索来说，增加了recall，降低了precision。英语中最流行的两个stemming算法是`Porter`和`Snowball`。\n",
    "* lemmatization：lemma意为词条，与词义有关。由于它考虑了词义，而不仅仅是后缀，因此大部分时候好于stemming，而且lemma都是合法的词。因此建议先lemmatization，再stemming。\n",
    "\n",
    "三种方法都可以缩小词汇量，增加recall，降低precision。底线是，如果不是数据量很有限，避免使用：）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('house', 'doctor house call')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stem(phrase):\n",
    "    return ' '.join([re.findall('^(.*ss|.*?)(s)?$', word)[0][0].strip(\"'\") for word in phrase.lower().split()])\n",
    "\n",
    "stem('houses'), stem(\"Doctor House's calls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dish washer wash dish'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "sent = \"dish washer's washed dishes\"\n",
    "' '.join([stemmer.stem(w).strip(\"'\") for w in sent.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/andersc/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 情感（Sentiment）\n",
    "\n",
    "以上将文本分割为token或n-grams，它们多少会包含一点“信息”，信息的一个主要方面是其表达的情感——总体的感觉或情绪。**情感分析（sentiment analysis）**是NLP的常见应用之一。\n",
    "\n",
    "比如一家公司希望了解用户如何看到其产品。一种方式是让用户打星（豆瓣），另一更自然的方式是让用户输入一段文字作为评论，此时情感分析就很有用了。\n",
    "\n",
    "情感分析有两种方法：\n",
    "\n",
    "* 基于人工规则：使用启发式（heuristics）方法\n",
    "* 机器学习方法：常使用来自tweet等网站的数据，因为那些数据会带有hashtag。对于豆瓣或淘宝，可以结合用户的评论文字与其打星。\n",
    "\n",
    "### 2.3.1 VADER - 基于规则\n",
    "\n",
    "VADER是较早出现的基于规则的情感分析工具。\n",
    "\n",
    "[vaderSentiment](https://github.com/cjhutto/vaderSentiment)\n",
    "\n",
    "### 2.3.2 Naive Bayes\n",
    "\n",
    "NB相当于通过数据学习出类似于VADER中的系数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.27</td>\n",
       "      <td>The Rock is destined to be the 21st Century's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.53</td>\n",
       "      <td>The gorgeously elaborate continuation of ''The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.60</td>\n",
       "      <td>Effective but too tepid biopic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.47</td>\n",
       "      <td>If you sometimes like to go to the movies to h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.73</td>\n",
       "      <td>Emerges as something rare, an issue movie that...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment                                               text\n",
       "id                                                              \n",
       "1        2.27  The Rock is destined to be the 21st Century's ...\n",
       "2        3.53  The gorgeously elaborate continuation of ''The...\n",
       "3       -0.60                     Effective but too tepid biopic\n",
       "4        1.47  If you sometimes like to go to the movies to h...\n",
       "5        1.73  Emerges as something rare, an issue movie that..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies = pd.read_csv('movieReviewSnippets_GroundTruth.txt', delimiter='\\t', index_col='id', error_bad_lines=False)\n",
    "movies.head().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10605.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentiment\n",
       "count   10605.00\n",
       "mean        0.00\n",
       "std         1.92\n",
       "min        -3.88\n",
       "25%        -1.77\n",
       "50%        -0.08\n",
       "75%         1.83\n",
       "max         3.94"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10605, 20756)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from nltk.tokenize import casual_tokenize\n",
    "\n",
    "bow = []\n",
    "for text in movies.text:\n",
    "    bow.append(Counter(casual_tokenize(text)))\n",
    "    \n",
    "df_bows = pd.DataFrame.from_records(bow)\n",
    "df_bows = df_bows.fillna(0).astype(int)\n",
    "df_bows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>!</th>\n",
       "      <th>\"</th>\n",
       "      <th>#</th>\n",
       "      <th>$</th>\n",
       "      <th>%</th>\n",
       "      <th>&amp;</th>\n",
       "      <th>'</th>\n",
       "      <th>(</th>\n",
       "      <th>)</th>\n",
       "      <th>*</th>\n",
       "      <th>...</th>\n",
       "      <th>zips</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoning</th>\n",
       "      <th>zzzzzzzzz</th>\n",
       "      <th>½</th>\n",
       "      <th>élan</th>\n",
       "      <th>–</th>\n",
       "      <th>’</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 20756 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   !  \"  #  $  %  &  '  (  )  *  ...  zips  zombie  zombies  zone  zoning  \\\n",
       "0  0  0  0  0  0  0  4  0  0  0  ...     0       0        0     0       0   \n",
       "1  0  0  0  0  0  0  4  0  0  0  ...     0       0        0     0       0   \n",
       "2  0  0  0  0  0  0  0  0  0  0  ...     0       0        0     0       0   \n",
       "3  0  0  0  0  0  0  0  0  0  0  ...     0       0        0     0       0   \n",
       "4  0  0  0  0  0  0  0  0  0  0  ...     0       0        0     0       0   \n",
       "\n",
       "   zzzzzzzzz  ½  élan  –  ’  \n",
       "0          0  0     0  0  0  \n",
       "1          0  0     0  0  0  \n",
       "2          0  0     0  0  0  \n",
       "3          0  0     0  0  0  \n",
       "4          0  0     0  0  0  \n",
       "\n",
       "[5 rows x 20756 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bows.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb = nb.fit(df_bows, movies.sentiment > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['pred'] = nb.predict_proba(df_bows)[:, 1] * 8 - 4\n",
    "movies['error'] = (movies.pred - movies.sentiment).abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.error.mean().round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['pos'] = (movies.sentiment > 0).astype(int)\n",
    "movies['pred_pos'] = (movies.pred > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>pred</th>\n",
       "      <th>error</th>\n",
       "      <th>pos</th>\n",
       "      <th>pred_pos</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.266667</td>\n",
       "      <td>The Rock is destined to be the 21st Century's ...</td>\n",
       "      <td>2.511515</td>\n",
       "      <td>0.244848</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.533333</td>\n",
       "      <td>The gorgeously elaborate continuation of ''The...</td>\n",
       "      <td>3.999904</td>\n",
       "      <td>0.466571</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.600000</td>\n",
       "      <td>Effective but too tepid biopic</td>\n",
       "      <td>-3.655976</td>\n",
       "      <td>3.055976</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.466667</td>\n",
       "      <td>If you sometimes like to go to the movies to h...</td>\n",
       "      <td>1.940954</td>\n",
       "      <td>0.474287</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.733333</td>\n",
       "      <td>Emerges as something rare, an issue movie that...</td>\n",
       "      <td>3.910373</td>\n",
       "      <td>2.177040</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.533333</td>\n",
       "      <td>The film provides some great insight into the ...</td>\n",
       "      <td>3.995188</td>\n",
       "      <td>1.461854</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.466667</td>\n",
       "      <td>Offers that rare combination of entertainment ...</td>\n",
       "      <td>3.960466</td>\n",
       "      <td>1.493799</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.266667</td>\n",
       "      <td>Perhaps no picture ever made has more literall...</td>\n",
       "      <td>-1.918701</td>\n",
       "      <td>3.185368</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment                                               text      pred  \\\n",
       "id                                                                           \n",
       "1    2.266667  The Rock is destined to be the 21st Century's ...  2.511515   \n",
       "2    3.533333  The gorgeously elaborate continuation of ''The...  3.999904   \n",
       "3   -0.600000                     Effective but too tepid biopic -3.655976   \n",
       "4    1.466667  If you sometimes like to go to the movies to h...  1.940954   \n",
       "5    1.733333  Emerges as something rare, an issue movie that...  3.910373   \n",
       "6    2.533333  The film provides some great insight into the ...  3.995188   \n",
       "7    2.466667  Offers that rare combination of entertainment ...  3.960466   \n",
       "8    1.266667  Perhaps no picture ever made has more literall... -1.918701   \n",
       "\n",
       "       error  pos  pred_pos  \n",
       "id                           \n",
       "1   0.244848    1         1  \n",
       "2   0.466571    1         1  \n",
       "3   3.055976    0         0  \n",
       "4   0.474287    1         1  \n",
       "5   2.177040    1         1  \n",
       "6   1.461854    1         1  \n",
       "7   1.493799    1         1  \n",
       "8   3.185368    1         0  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9344648750589345"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = (movies.pos == movies.pred_pos).sum() / len(movies)\n",
    "precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要注意的点：\n",
    "\n",
    "* NB对于否定词的处理不够好，需要引入n-grams；\n",
    "* 需要区分开训练集和测试集\n",
    "* 在movies上训练后，对于其它领域的效果会差很多"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3、词的数学（TF-IDF向量）\n",
    "\n",
    "* 统计词频以分析语义\n",
    "* 通过Zipf法则预测词的概率\n",
    "* 词的向量表示及用法\n",
    "* 使用IDF寻找“相关的”文档\n",
    "* 使用余弦相似度估计文档的相似度\n",
    "\n",
    "在将文本分割为token后，可以想象的是，对于一个文档而言，不同token的重要性不同。本章将讨论衡量token重要性的一种方法，该方法在搜索引擎和垃圾邮件过滤上是传统的主流方法。\n",
    "\n",
    "BOW向量基本上是**bit vector**，主要表示一个token是否出现。如果有办法将token表示为连续值，那么会有更有趣的数学操作可做。接下来将讨论三种方法：\n",
    "\n",
    "* BOW：词频\n",
    "* Bags of n-grams：\n",
    "* TF-IDF\n",
    "\n",
    "这三种方法都是基于频率的，都属于**浅层（shallow）**方法，但在某些应用中，它们已经足够强大，如邮件过滤和情感分析。\n",
    "\n",
    "## 3.1 词袋模型\n",
    "\n",
    "在上面的NB例子中，已经用到了词袋模型，它不是最简单的`one-hot`，而是包含了词频，其隐含的意思是，频率越高者对文档意义贡献越大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'the': 4,\n",
       "         'faster': 3,\n",
       "         'harry': 2,\n",
       "         'got': 1,\n",
       "         'to': 1,\n",
       "         'store': 1,\n",
       "         ',': 3,\n",
       "         'would': 1,\n",
       "         'get': 1,\n",
       "         'home': 1,\n",
       "         '.': 1})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "sent = 'The faster Harry got to the store, the faster Harry, the faster, would get home.'\n",
    "tokens = tokenizer.tokenize(sent.lower())\n",
    "\n",
    "bow = Counter(tokens)\n",
    "bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词在文档中的频率称为*term frequency*，简称*TF*。由于不同文档间的长度相差会很大，因此通常要使用$count(term) / len(doc)$表示一个term的TF值。下面来看一个更大的文档："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "kite_text = \"\"\"\n",
    "A kite is traditionally a tethered heavier-than-air craft with wing surfaces that react\n",
    "against the air to create lift and drag. A kite consists of wings, tethers, and anchors.\n",
    "Kites often have a bridle to guide the face of the kite at the correct angle so the wind\n",
    "can lift it. A kite’s wing also may be so designed so a bridle is not needed; when\n",
    "kiting a sailplane for launch, the tether meets the wing at a single point. A kite may\n",
    "have fixed or moving anchors. Untraditionally in technical kiting, a kite consists of\n",
    "tether-set-coupled wing sets; even in technical kiting, though, a wing in the system is\n",
    "still often called the kite.\n",
    "\n",
    "The lift that sustains the kite in flight is generated when air flows around the kite’s\n",
    "surface, producing low pressure above and high pressure below the wings. The\n",
    "interaction with the wind also generates horizontal drag along the direction of the\n",
    "wind. The resultant force vector from the lift and drag force components is opposed\n",
    "by the tension of one or more of the lines or tethers to which the kite is attached. The\n",
    "anchor point of the kite line may be static or moving (such as the towing of a kite by\n",
    "a running person, boat, free-falling anchors as in paragliders and fugitive parakites\n",
    "or vehicle).\n",
    "\n",
    "The same principles of fluid flow apply in liquids and kites are also used under water.\n",
    "A hybrid tethered craft comprising both a lighter-than-air balloon as well as a kite\n",
    "lifting surface is called a kytoon.\n",
    "\n",
    "Kites have a long and varied history and many different types are flown\n",
    "individually and at festivals worldwide. Kites may be flown for recreation, art or\n",
    "other practical uses. Sport kites can be flown in aerial ballet, sometimes as part of a\n",
    "competition. Power kites are multi-line steerable kites designed to generate large forces\n",
    "which can be used to power activities such as kite surfing, kite landboarding, kite\n",
    "fishing, kite buggying and a new trend snow kiting. Even Man-lifting kites have\n",
    "been made.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 26), ('a', 20), ('kite', 16), (',', 14), ('and', 10)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(kite_text.lower())\n",
    "token_counts = Counter(tokens)\n",
    "token_counts.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kite', 16), (',', 14), ('kites', 8), ('wing', 5), ('lift', 4)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "tokens = [t for t in tokens if t not in stopwords]\n",
    "token_counts = Counter(tokens)\n",
    "token_counts.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [\"The faster Harry got to the store, the faster and faster Harry would get home.\",\n",
    "        \"Harry is hairy and faster than Jill.\",\n",
    "        \"Jill is not as hairy as Harry.\"]\n",
    "\n",
    "doc_tokens = []\n",
    "for doc in docs:\n",
    "    doc_tokens.append(sorted(tokenizer.tokenize(doc.lower())))\n",
    "    \n",
    "len(doc_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_doc_tokens = sum(doc_tokens, [])\n",
    "len(all_doc_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon = sorted(set(all_doc_tokens))\n",
    "len(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "zero_vector = OrderedDict((token, 0) for token in lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(',', 0.05555555555555555),\n",
       "             ('.', 0.05555555555555555),\n",
       "             ('and', 0.05555555555555555),\n",
       "             ('as', 0),\n",
       "             ('faster', 0.16666666666666666),\n",
       "             ('get', 0.05555555555555555),\n",
       "             ('got', 0.05555555555555555),\n",
       "             ('hairy', 0),\n",
       "             ('harry', 0.1111111111111111),\n",
       "             ('home', 0.05555555555555555),\n",
       "             ('is', 0),\n",
       "             ('jill', 0),\n",
       "             ('not', 0),\n",
       "             ('store', 0.05555555555555555),\n",
       "             ('than', 0),\n",
       "             ('the', 0.16666666666666666),\n",
       "             ('to', 0.05555555555555555),\n",
       "             ('would', 0.05555555555555555)])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "doc_vectors = []\n",
    "for doc in docs:\n",
    "    vec = copy.copy(zero_vector)\n",
    "    tokens = tokenizer.tokenize(doc.lower())\n",
    "    token_counts = Counter(tokens)\n",
    "    for k, v in token_counts.items():\n",
    "        vec[k] = v / len(lexicon)\n",
    "    doc_vectors.append(vec)\n",
    "    \n",
    "doc_vectors[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上操作，对于一个corpus，为其中的每个文档建立了向量表示。不过，`vec[k] = v / len(lexicon)`应该是有问题的？\n",
    "\n",
    "### 3.2.1 向量空间\n",
    "\n",
    "一个**向量空间**是其中所有向量的集合，如二维向量空间对应平面内的所有向量。（回忆一哈线性代数什么的）\n",
    "\n",
    "NLP中，向量空间的维度是词汇表的大小，一般记为$K$或$|V|$。衡量向量的相似度，主要通过它们的方向，而非距离（欧几里得距离等），而方向就引出“余弦相似度”。\n",
    "\n",
    "如果余弦相似度接近1，表明两个文档不仅包含类似的词，各个词的出现比例也是接近的。如果为0，则说明两个文档没有任何共同的词（但并不全然说明两个文档语义无关）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/andersc/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1161192\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Fulton',\n",
       " 'County',\n",
       " 'Grand',\n",
       " 'Jury',\n",
       " 'said',\n",
       " 'Friday',\n",
       " 'an',\n",
       " 'investigation',\n",
       " 'of']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "brown_words = brown.words()\n",
    "print(len(brown_words))\n",
    "brown_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Topic Modeling\n",
    "\n",
    "单纯依靠$TF$不能了解一个词**相对于**其它文档对于当前文档的重要性。一个词在当前文档频率较高，但如果在多数文档中都是如此，那么它也就没那么重要了。这是就需要$IDF$了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "kite_into = kite_text\n",
    "kite_history = \"\"\"Kites were invented in China, where materials ideal for kite building were readily\n",
    "available: silk fabric for sail material; fine, high-tensile-strength silk for flying line;\n",
    "and resilient bamboo for a strong, lightweight framework.\n",
    "\n",
    "The kite has been claimed as the invention of the 5th-century BC Chinese\n",
    "philosophers Mozi (also Mo Di) and Lu Ban (also Gongshu Ban). By 549 AD\n",
    "paper kites were certainly being flown, as it was recorded that in that year a paper\n",
    "kite was used as a message for a rescue mission. Ancient and medieval Chinese\n",
    "sources describe kites being used for measuring distances, testing the wind, lifting\n",
    "men, signaling, and communication for military operations. The earliest known\n",
    "Chinese kites were flat (not bowed) and often rectangular. Later, tailless kites\n",
    "incorporated a stabilizing bowline. Kites were decorated with mythological motifs\n",
    "and legendary figures; some were fitted with strings and whistles to make musical\n",
    "sounds while flying. From China, kites were introduced to Cambodia, Thailand,\n",
    "India, Japan, Korea and the western world.\n",
    "\n",
    "After its introduction into India, the kite further evolved into the fighter kite, known\n",
    "as the patang in India, where thousands are flown every year on festivals such as\n",
    "Makar Sankranti.\n",
    "\n",
    "Kites were known throughout Polynesia, as far as New Zealand, with the\n",
    "assumption being that the knowledge diffused from China along with the people.\n",
    "Anthropomorphic kites made from cloth and wood were used in religious ceremonies\n",
    "to send prayers to the gods. Polynesian kite traditions are used by anthropologists get\n",
    "an idea of early “primitive” Asian traditions that are believed to have at one time\n",
    "existed in Asia.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(365, 297)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "intro_tokens = tokenizer.tokenize(kite_into.lower())\n",
    "hist_tokens = tokenizer.tokenize(kite_history.lower())\n",
    "\n",
    "intro_total, hist_total = len(intro_tokens), len(hist_tokens)\n",
    "intro_total, hist_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.04, 0.02)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intro_tf, hist_tf = {}, {}\n",
    "intro_counts, hist_counts = Counter(intro_tokens), Counter(hist_tokens)\n",
    "\n",
    "intro_tf['kite'] = intro_counts['kite'] / intro_total\n",
    "hist_tf['kite'] = hist_counts['kite'] / hist_total\n",
    "\n",
    "round(intro_tf['kite'], 2), round(hist_tf['kite'], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原始的IDF值会有一些问题。比如在一个很大的语料库中，一个出现1次的词IDF值是出现10次的词的10倍，但实际上两者都属于“很高”的那一类，而非相差如此悬殊。因此一般做法是取log。\n",
    "\n",
    "看起来定义极为简单的$TF-IDF$是搜索引擎的基础。这里实现一个基本的计算方法：\n",
    "\n",
    "$$ \\frac{f_{t,d}}{\\sum_{t' \\in d}^{} f_{t',d} } * log(\\frac{N}{1+n_t}) $$\n",
    "\n",
    "这一算式计算文档d中term t的TF-IDF值，生产环境中可考虑sklean等库的实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33, 18)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [\"The faster Harry got to the store, the faster and faster Harry would get home.\",\n",
    "        \"Harry is hairy and faster than Jill.\",\n",
    "        \"Jill is not as hairy as Harry.\"]\n",
    "\n",
    "doc_tokens = []\n",
    "for doc in docs:\n",
    "    doc_tokens.append(sorted(tokenizer.tokenize(doc.lower())))\n",
    "    \n",
    "all_doc_tokens = sum(doc_tokens, [])\n",
    "lexicon = sorted(set(all_doc_tokens))\n",
    "len(all_doc_tokens), len(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(',', 0.08154672712469944),\n",
       "             ('.', 0.016922474850104754),\n",
       "             ('and', 0.04077336356234972),\n",
       "             ('as', 0),\n",
       "             ('faster', 0.12232009068704917),\n",
       "             ('get', 0.08154672712469944),\n",
       "             ('got', 0.08154672712469944),\n",
       "             ('hairy', 0),\n",
       "             ('harry', 0.03384494970020951),\n",
       "             ('home', 0.08154672712469944),\n",
       "             ('is', 0),\n",
       "             ('jill', 0),\n",
       "             ('not', 0),\n",
       "             ('store', 0.08154672712469944),\n",
       "             ('than', 0),\n",
       "             ('the', 0.24464018137409835),\n",
       "             ('to', 0.08154672712469944),\n",
       "             ('would', 0.08154672712469944)])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from math import log\n",
    "\n",
    "zero_vector = OrderedDict((token, 0) for token in lexicon)\n",
    "\n",
    "import copy\n",
    "\n",
    "N = len(doc_tokens) + 1\n",
    "all_token_counts = Counter()\n",
    "for tokens in doc_tokens:\n",
    "    all_token_counts.update(set(tokens))\n",
    "\n",
    "doc_vectors = []\n",
    "for tokens in doc_tokens:\n",
    "    n_doc_tokens = len(tokens)\n",
    "    vec = copy.copy(zero_vector)\n",
    "    token_counts = Counter(tokens)\n",
    "    for k, v in token_counts.items():\n",
    "        tf = v / n_doc_tokens\n",
    "        idf = log(N / all_token_counts[k])\n",
    "        vec[k] = tf * idf\n",
    "    doc_vectors.append(vec)\n",
    "    \n",
    "doc_vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(',', 0),\n",
       "             ('.', 0),\n",
       "             ('and', 0),\n",
       "             ('as', 0),\n",
       "             ('faster', 0),\n",
       "             ('get', 0.06301338005090412),\n",
       "             ('got', 0),\n",
       "             ('hairy', 0),\n",
       "             ('harry', 0),\n",
       "             ('home', 0),\n",
       "             ('is', 0),\n",
       "             ('jill', 0),\n",
       "             ('not', 0),\n",
       "             ('store', 0.06301338005090412),\n",
       "             ('than', 0),\n",
       "             ('the', 0.06301338005090412),\n",
       "             ('to', 0.12602676010180824),\n",
       "             ('would', 0)])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"How long does it take to get to the store?\"\n",
    "\n",
    "tokens = tokenizer.tokenize(query.lower())\n",
    "n_query_tokens = len(tokens)\n",
    "\n",
    "query_vec = copy.copy(zero_vector)\n",
    "query_token_counts = Counter(tokens)\n",
    "for k, v in query_token_counts.items():\n",
    "    if k in all_token_counts:\n",
    "        tf = v / n_query_tokens\n",
    "        idf = log(N / (1 + all_token_counts[k]))\n",
    "        query_vec[k] = tf * idf\n",
    "    \n",
    "query_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def cosine_sim(v1, v2):\n",
    "    v1 = [v for v in v1.values()]\n",
    "    v2 = [v for v in v2.values()]\n",
    "    \n",
    "    dot_prod = 0\n",
    "    for i, v in enumerate(v1):\n",
    "        dot_prod += v * v2[i]\n",
    "    \n",
    "    mag1 = math.sqrt(sum([x**2 for x in v1]))\n",
    "    mag2 = math.sqrt(sum([x**2 for x in v2]))\n",
    "    \n",
    "    return dot_prod / (mag1 * mag2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6115759372893524\n",
      "1 0.0\n",
      "2 0.0\n"
     ]
    }
   ],
   "source": [
    "for i, doc_vec in enumerate(doc_vectors):\n",
    "    print(i, cosine_sim(query_vec, doc_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 18)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query_vec), len(doc_vectors[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4、寻找词频中的语义（语义分析）\n",
    "\n",
    "* 分析语义，创建主题向量\n",
    "* 使用主题向量进行语义搜索\n",
    "* 使用语义组件作为pipeline的特征\n",
    "* 探索高维向量空间\n",
    "\n",
    "如果你知道想搜索的词具体是什么，那么TF-IDF是不错的选择，但很多时候并不清楚要搜什么。\n",
    "\n",
    "人们开发了*LSA（Latent Semantic Analysis）*，LSA不仅可以表示词的向量还可以表示整个文档的向量。这一章即介绍**topic vector**。如此，可以进行语义搜索，而非词的搜索。另外，还可以识别出能够表示整个文档的词或n-grams（关键词提取）。\n",
    "\n",
    "## 4.1 从词频到topic score\n",
    "\n",
    "TF-IDF基于term的准确拼写，即使考虑到大小写、stemming、lemmatization之类，亦是如此。问题是，拼写类似的词语义未必类似，而同义之词拼写可能会差得很远。同时stemming不能处理好反义词的情况。\n",
    "\n",
    "### 4.1.2 主题向量（topic vector）\n",
    "\n",
    "对TF-IDF向量进行相加操作，只是得到另一个关于“词频”的向量，与语义不尽相关。我们需要一种更为“紧致的”向量表示——word-topic vector。对于这种向量，加减操作就蕴含着更多了。\n",
    "\n",
    "注：**polysemy**：一词多义。\n",
    "\n",
    "* Homonym：同形（音）异义词\n",
    "* Zeugma：同一句中，一词具有多义\n",
    "\n",
    "对于大型的corpus，TF-IDF向量可能高达百万维，现在需要将其压缩为数百维，同时向量还要能表示一个词对于某个主题的贡献为多少。\n",
    "\n",
    "### 4.1.3 思想实验\n",
    "\n",
    "假设有一个小型语料库，其中文档是关于三个已知主题的，那么可以为每个term分配权重，表示这个词对每个主题的贡献是多少。\n",
    "\n",
    "### 4.1.4 一个算法\n",
    "\n",
    "需要一个算法将TF-IDF向量转换为topic向量。\n",
    "\n",
    "LSA算法分析TF-IDF矩阵将词收集到主题中，也可以用于bow，但前者效果更好。由于主题数量一般要远小于词汇大小，因此LSA常常被看作一种降维技术，奇妙的是它与另一种降维技术PCA遵循同样的数学原理。\n",
    "\n",
    "LSA有两个近似算法：\n",
    "\n",
    "* LDA：Linear Discriminant Analysis（线性判别分析）\n",
    "* LDiA：Latent Dirichlet Allocation\n",
    "\n",
    "### 4.1.5 An LDA Classifier\n",
    "\n",
    "LDA是最直接、快速的降维和分类模型之一。它是监督式算法，因此需要标注数据，但所需数据远少于其它facier的算法。这里用它来解决一个二分类问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-22 00:55:42,872 WARNING:nlpia.constants:106:            <module> Starting logger in nlpia.constants...\n"
     ]
    }
   ],
   "source": [
    "from nlpia.data.loaders import get_data\n",
    "\n",
    "sms = get_data('sms-spam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = [f'sms{i}{\"!\"*j}' for (i, j) in enumerate(sms.spam)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4837"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms = pd.DataFrame(sms.values, columns=sms.columns, index=index)\n",
    "sms['spam'] = sms.spam.astype(int)\n",
    "len(sms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "638"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.spam.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spam</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sms0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms2!</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       spam                                               text\n",
       "sms0      0  Go until jurong point, crazy.. Available only ...\n",
       "sms1      0                      Ok lar... Joking wif u oni...\n",
       "sms2!     1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "sms3      0  U dun say so early hor... U c already then say...\n",
       "sms4      0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4837, 9232)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize.casual import casual_tokenize\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=casual_tokenize)\n",
    "tfidf = tfidf_vectorizer.fit_transform(raw_documents=sms.text).toarray()\n",
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特征数远超文档数，更远超spam数量，因此模型没有足够信息确定文档是否为spam，这时NB分类器不敷使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.06, 0.  , 0.  , ..., 0.  , 0.  , 0.  ])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = sms.spam.astype(bool).values\n",
    "spam_centroid = tfidf[mask].mean(axis=0)\n",
    "spam_centroid.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02, 0.01, 0.  , ..., 0.  , 0.  , 0.  ])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham_centroid = tfidf[~mask].mean(axis=0)\n",
    "ham_centroid.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01, -0.02,  0.04, ..., -0.01, -0.  ,  0.  ])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# line between centroids\n",
    "spamminess_score = tfidf.dot(spam_centroid - ham_centroid)\n",
    "spamminess_score.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spam</th>\n",
       "      <th>lda_predict</th>\n",
       "      <th>lda_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sms0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms2!</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms5!</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       spam  lda_predict  lda_score\n",
       "sms0      0            0       0.23\n",
       "sms1      0            0       0.18\n",
       "sms2!     1            1       0.72\n",
       "sms3      0            0       0.18\n",
       "sms4      0            0       0.29\n",
       "sms5!     1            1       0.55"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "sms['lda_score'] = MinMaxScaler().fit_transform(spamminess_score.reshape(-1, 1))\n",
    "sms['lda_predict'] = (sms.lda_score > 0.5).astype(int)\n",
    "sms['spam lda_predict lda_score'.split()].round(2).head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.977"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1. - (sms.spam - sms.lda_predict).abs().sum() / len(sms)).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
