{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5、初识神经网络（感知机和反向传播）\n",
    "\n",
    "* 学习NN（Neural Network）的历史\n",
    "* 层级感知机\n",
    "* 理解反向传播\n",
    "* “turn on” NN\n",
    "* 用Keras实现基本的NN\n",
    "\n",
    "反向传播（backpropagation），是NN的数学基础。\n",
    "\n",
    "## 5.1 NN的要素\n",
    "\n",
    "Rosenblatt的项目原本是“教”机器识别图像。\n",
    "\n",
    "### 5.1.2 基本的感知机\n",
    "\n",
    "![Basic](basic_perceptron.png)\n",
    "\n",
    "这里的$x_i$表示一个feature，所有的特征构成特征向量$X$，每个特征有对应的权重（weight）$w_i$，加上一个偏差（bias），它们的和输入一个“激活函数（Activation Function）”，激活函数输出1或0。\n",
    "\n",
    "偏差的存在是因为，神经元需要能够处理所有输入为0的情况，因为此时的加权和（即features和weights的点积）总是为0。\n",
    "\n",
    "以这样的视角来查看感知机，它是非常简单的定义。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.674"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs = [1, .2, .1, .05, .2]\n",
    "weights = [.2, .12, .4, .6, .9]\n",
    "\n",
    "input_vec = np.array(inputs)\n",
    "weight_vec = np.array(weights)\n",
    "bias_weight = .2\n",
    "\n",
    "activatin_level = np.dot(input_vec, weight_vec) + (1 * bias_weight)\n",
    "activatin_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def act_func(level):\n",
    "    threshold = 0.5\n",
    "    return 1 if level > threshold else 0\n",
    "\n",
    "perceptron_output = act_func(activatin_level)\n",
    "perceptron_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由以上运算过程可知，如果有了权重向量和激活函数，那么对于一个输入，就可以确定出输出是什么。但权重其实尚未确定出来，实际上它们才是“学习”的结果。\n",
    "\n",
    "感知机学习时，每次根据预测结果与实际结果的差异对权重做细微的调整。它的起点一般是**随机值**，通常选自正态分布。有了足够多输入后，系统将能够“学习”到最佳参数。一个调整的示例如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.2, 0.12, 0.4, 0.6, 0.9],\n",
       " [-0.8, -0.08000000000000002, 0.30000000000000004, 0.5499999999999999, 0.7])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 期望结果不同，因此调整值\n",
    "expected_output = 0\n",
    "\n",
    "new_weights = []\n",
    "for i, x in enumerate(inputs):\n",
    "    new_weights.append(weights[i] + (expected_output - perceptron_output) * x)\n",
    "weight_vec = np.array(new_weights)\n",
    "\n",
    "weights, new_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.41850000000000004"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activatin_level = np.dot(input_vec, weight_vec) + (1 * bias_weight)\n",
    "activatin_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron_output = act_func(activatin_level)\n",
    "perceptron_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过调整，对于该输入，其输出正确了：）下面来解决经典的“OR”问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights [0.00049163 0.00084834]\n",
      "bias weight 0.0004350974258090421\n"
     ]
    }
   ],
   "source": [
    "# inputs\n",
    "inputs = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "expected = [0, 1, 1, 1]\n",
    "act_threshold = 0.5\n",
    "\n",
    "from random import random\n",
    "\n",
    "# init weights\n",
    "weights = np.random.random(2) / 1000\n",
    "print('weights', weights)\n",
    "\n",
    "bias_weight = np.random.random() / 1000\n",
    "print('bias weight', bias_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 correct answers out of 4, for epoch 0\n",
      "2 correct answers out of 4, for epoch 1\n",
      "3 correct answers out of 4, for epoch 2\n",
      "4 correct answers out of 4, for epoch 3\n",
      "4 correct answers out of 4, for epoch 4\n"
     ]
    }
   ],
   "source": [
    "# 初始化接近原点的weights，开始训练\n",
    "for epoch in range(5):\n",
    "    correct_cnt = 0\n",
    "    for idx, sample in enumerate(inputs):\n",
    "        input_vec = np.array(sample)\n",
    "        act_level = np.dot(input_vec, weights) + (bias_weight * 1)\n",
    "        if act_level > act_threshold:\n",
    "            output = 1\n",
    "        else:\n",
    "            output = 0\n",
    "        if output == expected[idx]:\n",
    "            correct_cnt += 1\n",
    "        # 调整权重，weights与bias方式一致\n",
    "        new_weights = []\n",
    "        for i, x in enumerate(sample):\n",
    "            new_weights.append(weights[i] + (expected[idx] - output) * x)\n",
    "        bias_weight = bias_weight + (expected[idx] - output) * 1\n",
    "        weights = np.array(new_weights)\n",
    "    print(f'{correct_cnt} correct answers out of 4, for epoch {epoch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过四轮学习，结果就全对了，后面再学习也不再有任何帮助，因为weights不会再有调整。这种情况称为**收敛（convergence）**。模型的误差函数如果有最小值，那么它可说是收敛的。现实情况下，不是每个函数都是收敛的。\n",
    "\n",
    "基本感知机的固有问题是，它只能处理线性可分的数据。按上述计算过程来看，感知机只能做“线性回归”，故不能描述非线性关系。\n",
    "\n",
    "单个感知机能力有限，但如果组合多个感知机，情况就变化了。（Rumelhardt McClelland）他们重新采用了一个古老的方法：反向传播。\n",
    "\n",
    "对神经网络算法来说，即使它可以解决复杂的（非线性）问题，很长时间里，它也是过于高昂的算力，由此显得很不practical，在1990s到2010期间进入了第二次AI Winter。之后在算力、反向传播算法、大规模数据（猫、狗标注数据集之类）这些条件同时具备之后，NN再次返场。\n",
    "\n",
    "### 5.1.3 损失函数（cost function / loss function）\n",
    "\n",
    "通过定义损失函数，训练NN的目标即是：寻求使得损失函数具有最小值的参数。\n",
    "\n",
    "**神经网络（Neural Network）**，是一个**神经元（Neuron）**的集合，其中的部分神经元之间建有连接。目前所用的是full connected network，即输入的每一个元素，会连接到下一层的每一个神经元。\n",
    "\n",
    "### 5.1.4 激活函数（Activation Function）\n",
    "\n",
    "目前使用的激活函数是阶跃函数，但反向传播的激活函数需要时非线性和连续可导的。常用的一种激活函数是sigmoid。\n",
    "\n",
    "最简化的角度来看：输入所有训练数据，（总）误差会反向传播，以此更新每一个权重值，这样一个周期称为一个**epoch**。接下来可以重复执行多个epoch的工作。\n",
    "\n",
    "中间还有一个**learning rate**的概念。\n",
    "\n",
    "前面提到的最小值，是指对于所有输入得到总误差。\n",
    "\n",
    "### 5.1.5 学习方法\n",
    "\n",
    "输入所有数据，并以之调整权重的方法，称为*batch learning*，此方法可能只能找到局部最小值而非全局最小。有两种方法改进之：\n",
    "\n",
    "* stochastic gradient descent\n",
    "* mini-batch：更为常用\n",
    "\n",
    "### 5.1.6 Keras：NN in Python\n",
    "\n",
    "看下面XOR的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 10)                30        \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 11        \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 41\n",
      "Trainable params: 41\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "x_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_train = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "model = Sequential()\n",
    "n_neurons = 10\n",
    "\n",
    "model.add(Dense(n_neurons, input_dim=2))  # input_dim只需要在第一层定义，后续会自动计算\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dense(1))  # output layer\n",
    "model.add(Activation('sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解释：\n",
    "\n",
    "* Dense: fully connected layers\n",
    "* SDG: stochastic gd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       ],\n",
       "       [0.6537022 ],\n",
       "       [0.39829427],\n",
       "       [0.5253802 ]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd = SGD(lr=0.1)\n",
    "\n",
    "# compile创建初识模型\n",
    "model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# no learning yet\n",
    "model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "4/4 [==============================] - 0s 657us/step - loss: 0.5762 - acc: 0.7500\n",
      "Epoch 2/100\n",
      "4/4 [==============================] - 0s 524us/step - loss: 0.5752 - acc: 0.7500\n",
      "Epoch 3/100\n",
      "4/4 [==============================] - 0s 529us/step - loss: 0.5742 - acc: 0.7500\n",
      "Epoch 4/100\n",
      "4/4 [==============================] - 0s 625us/step - loss: 0.5732 - acc: 0.7500\n",
      "Epoch 5/100\n",
      "4/4 [==============================] - 0s 575us/step - loss: 0.5721 - acc: 0.7500\n",
      "Epoch 6/100\n",
      "4/4 [==============================] - 0s 534us/step - loss: 0.5711 - acc: 0.7500\n",
      "Epoch 7/100\n",
      "4/4 [==============================] - 0s 817us/step - loss: 0.5701 - acc: 0.7500\n",
      "Epoch 8/100\n",
      "4/4 [==============================] - 0s 560us/step - loss: 0.5691 - acc: 0.7500\n",
      "Epoch 9/100\n",
      "4/4 [==============================] - 0s 570us/step - loss: 0.5680 - acc: 0.7500\n",
      "Epoch 10/100\n",
      "4/4 [==============================] - 0s 536us/step - loss: 0.5670 - acc: 0.7500\n",
      "Epoch 11/100\n",
      "4/4 [==============================] - 0s 491us/step - loss: 0.5659 - acc: 0.7500\n",
      "Epoch 12/100\n",
      "4/4 [==============================] - 0s 655us/step - loss: 0.5649 - acc: 0.7500\n",
      "Epoch 13/100\n",
      "4/4 [==============================] - 0s 560us/step - loss: 0.5638 - acc: 0.7500\n",
      "Epoch 14/100\n",
      "4/4 [==============================] - 0s 514us/step - loss: 0.5627 - acc: 0.7500\n",
      "Epoch 15/100\n",
      "4/4 [==============================] - 0s 599us/step - loss: 0.5617 - acc: 0.7500\n",
      "Epoch 16/100\n",
      "4/4 [==============================] - 0s 746us/step - loss: 0.5606 - acc: 0.7500\n",
      "Epoch 17/100\n",
      "4/4 [==============================] - 0s 673us/step - loss: 0.5595 - acc: 0.7500\n",
      "Epoch 18/100\n",
      "4/4 [==============================] - 0s 531us/step - loss: 0.5584 - acc: 0.7500\n",
      "Epoch 19/100\n",
      "4/4 [==============================] - 0s 486us/step - loss: 0.5573 - acc: 0.7500\n",
      "Epoch 20/100\n",
      "4/4 [==============================] - 0s 523us/step - loss: 0.5562 - acc: 0.7500\n",
      "Epoch 21/100\n",
      "4/4 [==============================] - 0s 625us/step - loss: 0.5551 - acc: 0.7500\n",
      "Epoch 22/100\n",
      "4/4 [==============================] - 0s 765us/step - loss: 0.5539 - acc: 0.7500\n",
      "Epoch 23/100\n",
      "4/4 [==============================] - 0s 496us/step - loss: 0.5528 - acc: 0.7500\n",
      "Epoch 24/100\n",
      "4/4 [==============================] - 0s 542us/step - loss: 0.5517 - acc: 0.7500\n",
      "Epoch 25/100\n",
      "4/4 [==============================] - 0s 532us/step - loss: 0.5505 - acc: 0.7500\n",
      "Epoch 26/100\n",
      "4/4 [==============================] - 0s 587us/step - loss: 0.5494 - acc: 0.7500\n",
      "Epoch 27/100\n",
      "4/4 [==============================] - 0s 715us/step - loss: 0.5482 - acc: 0.7500\n",
      "Epoch 28/100\n",
      "4/4 [==============================] - 0s 608us/step - loss: 0.5471 - acc: 0.7500\n",
      "Epoch 29/100\n",
      "4/4 [==============================] - 0s 500us/step - loss: 0.5459 - acc: 0.7500\n",
      "Epoch 30/100\n",
      "4/4 [==============================] - 0s 560us/step - loss: 0.5447 - acc: 0.7500\n",
      "Epoch 31/100\n",
      "4/4 [==============================] - 0s 548us/step - loss: 0.5435 - acc: 0.7500\n",
      "Epoch 32/100\n",
      "4/4 [==============================] - 0s 530us/step - loss: 0.5424 - acc: 0.7500\n",
      "Epoch 33/100\n",
      "4/4 [==============================] - 0s 655us/step - loss: 0.5412 - acc: 0.7500\n",
      "Epoch 34/100\n",
      "4/4 [==============================] - 0s 477us/step - loss: 0.5400 - acc: 0.7500\n",
      "Epoch 35/100\n",
      "4/4 [==============================] - 0s 629us/step - loss: 0.5388 - acc: 0.7500\n",
      "Epoch 36/100\n",
      "4/4 [==============================] - 0s 593us/step - loss: 0.5375 - acc: 0.7500\n",
      "Epoch 37/100\n",
      "4/4 [==============================] - 0s 678us/step - loss: 0.5363 - acc: 0.7500\n",
      "Epoch 38/100\n",
      "4/4 [==============================] - 0s 523us/step - loss: 0.5351 - acc: 0.7500\n",
      "Epoch 39/100\n",
      "4/4 [==============================] - 0s 496us/step - loss: 0.5339 - acc: 0.7500\n",
      "Epoch 40/100\n",
      "4/4 [==============================] - 0s 536us/step - loss: 0.5326 - acc: 0.7500\n",
      "Epoch 41/100\n",
      "4/4 [==============================] - 0s 490us/step - loss: 0.5314 - acc: 0.7500\n",
      "Epoch 42/100\n",
      "4/4 [==============================] - 0s 608us/step - loss: 0.5301 - acc: 0.7500\n",
      "Epoch 43/100\n",
      "4/4 [==============================] - 0s 574us/step - loss: 0.5289 - acc: 0.7500\n",
      "Epoch 44/100\n",
      "4/4 [==============================] - 0s 462us/step - loss: 0.5276 - acc: 0.7500\n",
      "Epoch 45/100\n",
      "4/4 [==============================] - 0s 935us/step - loss: 0.5263 - acc: 0.7500\n",
      "Epoch 46/100\n",
      "4/4 [==============================] - 0s 494us/step - loss: 0.5250 - acc: 0.7500\n",
      "Epoch 47/100\n",
      "4/4 [==============================] - 0s 547us/step - loss: 0.5238 - acc: 0.7500\n",
      "Epoch 48/100\n",
      "4/4 [==============================] - 0s 505us/step - loss: 0.5225 - acc: 0.7500\n",
      "Epoch 49/100\n",
      "4/4 [==============================] - 0s 566us/step - loss: 0.5212 - acc: 0.7500\n",
      "Epoch 50/100\n",
      "4/4 [==============================] - 0s 811us/step - loss: 0.5199 - acc: 0.7500\n",
      "Epoch 51/100\n",
      "4/4 [==============================] - 0s 527us/step - loss: 0.5185 - acc: 0.7500\n",
      "Epoch 52/100\n",
      "4/4 [==============================] - 0s 505us/step - loss: 0.5172 - acc: 0.7500\n",
      "Epoch 53/100\n",
      "4/4 [==============================] - 0s 584us/step - loss: 0.5159 - acc: 0.7500\n",
      "Epoch 54/100\n",
      "4/4 [==============================] - 0s 576us/step - loss: 0.5146 - acc: 0.7500\n",
      "Epoch 55/100\n",
      "4/4 [==============================] - 0s 848us/step - loss: 0.5132 - acc: 0.7500\n",
      "Epoch 56/100\n",
      "4/4 [==============================] - 0s 487us/step - loss: 0.5119 - acc: 0.7500\n",
      "Epoch 57/100\n",
      "4/4 [==============================] - 0s 508us/step - loss: 0.5106 - acc: 0.7500\n",
      "Epoch 58/100\n",
      "4/4 [==============================] - 0s 522us/step - loss: 0.5092 - acc: 0.7500\n",
      "Epoch 59/100\n",
      "4/4 [==============================] - 0s 557us/step - loss: 0.5078 - acc: 0.7500\n",
      "Epoch 60/100\n",
      "4/4 [==============================] - 0s 473us/step - loss: 0.5065 - acc: 0.7500\n",
      "Epoch 61/100\n",
      "4/4 [==============================] - 0s 527us/step - loss: 0.5051 - acc: 1.0000\n",
      "Epoch 62/100\n",
      "4/4 [==============================] - 0s 591us/step - loss: 0.5037 - acc: 1.0000\n",
      "Epoch 63/100\n",
      "4/4 [==============================] - 0s 787us/step - loss: 0.5023 - acc: 1.0000\n",
      "Epoch 64/100\n",
      "4/4 [==============================] - 0s 508us/step - loss: 0.5009 - acc: 1.0000\n",
      "Epoch 65/100\n",
      "4/4 [==============================] - 0s 487us/step - loss: 0.4995 - acc: 1.0000\n",
      "Epoch 66/100\n",
      "4/4 [==============================] - 0s 614us/step - loss: 0.4981 - acc: 1.0000\n",
      "Epoch 67/100\n",
      "4/4 [==============================] - 0s 522us/step - loss: 0.4967 - acc: 1.0000\n",
      "Epoch 68/100\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.4953 - acc: 1.0000\n",
      "Epoch 69/100\n",
      "4/4 [==============================] - 0s 599us/step - loss: 0.4939 - acc: 1.0000\n",
      "Epoch 70/100\n",
      "4/4 [==============================] - 0s 575us/step - loss: 0.4925 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "4/4 [==============================] - 0s 491us/step - loss: 0.4910 - acc: 1.0000\n",
      "Epoch 72/100\n",
      "4/4 [==============================] - 0s 555us/step - loss: 0.4896 - acc: 1.0000\n",
      "Epoch 73/100\n",
      "4/4 [==============================] - 0s 525us/step - loss: 0.4882 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "4/4 [==============================] - 0s 558us/step - loss: 0.4867 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "4/4 [==============================] - 0s 569us/step - loss: 0.4853 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "4/4 [==============================] - 0s 541us/step - loss: 0.4838 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "4/4 [==============================] - 0s 567us/step - loss: 0.4823 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "4/4 [==============================] - 0s 612us/step - loss: 0.4809 - acc: 1.0000\n",
      "Epoch 79/100\n",
      "4/4 [==============================] - 0s 539us/step - loss: 0.4794 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "4/4 [==============================] - 0s 547us/step - loss: 0.4779 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "4/4 [==============================] - 0s 589us/step - loss: 0.4764 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "4/4 [==============================] - 0s 614us/step - loss: 0.4749 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "4/4 [==============================] - 0s 709us/step - loss: 0.4734 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "4/4 [==============================] - 0s 634us/step - loss: 0.4719 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "4/4 [==============================] - 0s 614us/step - loss: 0.4704 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "4/4 [==============================] - 0s 615us/step - loss: 0.4689 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "4/4 [==============================] - 0s 787us/step - loss: 0.4674 - acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/100\n",
      "4/4 [==============================] - 0s 583us/step - loss: 0.4659 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "4/4 [==============================] - 0s 637us/step - loss: 0.4644 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "4/4 [==============================] - 0s 655us/step - loss: 0.4628 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "4/4 [==============================] - 0s 778us/step - loss: 0.4613 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "4/4 [==============================] - 0s 583us/step - loss: 0.4598 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "4/4 [==============================] - 0s 559us/step - loss: 0.4582 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "4/4 [==============================] - 0s 545us/step - loss: 0.4567 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "4/4 [==============================] - 0s 573us/step - loss: 0.4551 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "4/4 [==============================] - 0s 603us/step - loss: 0.4536 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "4/4 [==============================] - 0s 610us/step - loss: 0.4520 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "4/4 [==============================] - 0s 577us/step - loss: 0.4504 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "4/4 [==============================] - 0s 556us/step - loss: 0.4489 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "4/4 [==============================] - 0s 634us/step - loss: 0.4473 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x641e77be0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NN不一定能够收敛，compile的结果有可能使得寻找全局最小值非常困难乃至不可能。如果发生这种情况，再次运行fit；或者重新compile后再fit。\n",
    "model.fit(x_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.24496274],\n",
       "       [0.69815433],\n",
       "       [0.59751886],\n",
       "       [0.4661598 ]], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0]], dtype=int32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "import h5py\n",
    "\n",
    "model_struct = model.to_json()\n",
    "\n",
    "with open('xor_model.json', 'w') as fout:\n",
    "    fout.write(model_struct)\n",
    "    \n",
    "# trained weights\n",
    "model.save_weights('xor_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.8 more things\n",
    "\n",
    "* 不同的激活函数（sigmoid，relu，hyperbolic tangent）\n",
    "* 选择不同的learning rate\n",
    "* 动态调整LR，使用momentum，寻找全局最小值\n",
    "* 使用dropout\n",
    "* 权重的regularization\n",
    "\n",
    "### 5.1.9 归一化\n",
    "\n",
    "normalization：将所有样本的所有特征值统一到某个特定的范围内。在NLP中，TF-IDF、one-hot、word2vec等已经属于normalized了，故不必太担心这一点。\n",
    "\n",
    "# 6、理解词向量（Word2vec）\n",
    "\n",
    "* 词向量是如何创建的\n",
    "* 使用预先训练的词向量模型\n",
    "* 使用词向量解决现实问题\n",
    "* 可视化之\n",
    "* 词嵌入的更多应用\n",
    "\n",
    "NLP近些年的进展中，最令人兴奋的点之一是词向量的”发现“。前面的章节中，我们忽略了词的语境，即词周围的词。BOW将文档中的所有词统统放入了一个大袋子，这一章的技术则使用小得多的袋子，一般不超过10个token。\n",
    "\n",
    "新介绍的技术能够识别出同义词、反义词，或同类词，如人、动物、地点等。而对于词、n-grams和文档的LSA，不能捕获词的意义，更不用说隐含的意思了。\n",
    "\n",
    "词向量是词义的数值表示，包含字面及隐含语义，因此可以说词向量捕获了词的”内涵（connotation）“，这些含义合并为一个dense vector。\n",
    "\n",
    "## 6.1 语义查询和类比（analogy）\n",
    "\n",
    "有时候你想搜索的那个特定词忘记了，只能输入ta的相关信息如：She invented something to do with physics in Europe in the early 20th century，通过Google搜索结果，你会找到答案是”Marie Curie“。\n",
    "\n",
    "而这种语义搜索可以通过词向量实现。我们要搜索的对象包含以下性质：女性、欧洲、物理学家、著名的，如果这些性质都有相应的向量，那么加起来得到”答案“的向量，然后可以去搜索最相似的那个。\n",
    "\n",
    "Who is to nuclear physics what Louis Pasteur is to germs?\n",
    "\n",
    "这是一个”之于“问题，即”谁之于核物理学，恰如巴斯德之于微生物学“？答案似乎应容易：\n",
    "\n",
    "`answer_vector = wv['Louis_Pasteur'] - wv['germs'] + wv['physics']`\n",
    "\n",
    "词向量善于解答此类问题。\n",
    "\n",
    "## 6.2 词向量\n",
    "\n",
    "2013年，Thomas Mikolov在Google发布了**Word2vec”。\n",
    "\n",
    "Word2vec的强悍之处是，它是无监督的，只需要大量的数据集，而现在这个时代，在Google这样的公司，数据不缺。\n",
    "\n",
    "Word2vec不需要标注，它预测的目标是一个词的临近词，这个正好在数据集之内。时间序列模型与NLP问题颇为相似，因为两者都是处理序列（词或数）。但预测本身不是Word2vec真正关心的，预测只是其手段，它关心的乃是词的内部表示。相比主题向量，词向量的表示捕获了更多的词义。\n",
    "\n",
    "PS：通过repredict输入来学习的模型称为**autoencoder**。\n",
    "\n",
    "Word2vec可能会学习到你意料不到的东西，比如每个词都会有一定的“geography”性质。语料库中的每个词都表示为向量，这类似于主题向量，惟词向量指向的语义更为具体和精确。\n",
    "\n",
    "关于Word2vec的直观理解是，将向量理解为一个权重或分数的列表，每个权重值都关联到一个特定维度的语义。\n",
    "\n",
    "Mikolov希望的是*vector-oriented reasoning*，简言之，可以对向量做加减运算。\n",
    "\n",
    "主题向量构建自整篇文档，适用于文档分类、语义搜索和聚类，但对于短语和复合词来说，不够准确。\n",
    "\n",
    "### 6.2.1 面向向量的推理\n",
    "\n",
    "Word2vec的第一次公开露面是在2013年的ACL，据称其在回答类比问题时的精确度是LSA模型的四倍。\n",
    "\n",
    "### 6.2.2 Word2vec的计算\n",
    "\n",
    "一般有两种方式：\n",
    "\n",
    "* skip-gram\n",
    "* continuous bag-of-words\n",
    "\n",
    "词向量的计算需要大量资源，不过也有公司提供了预训练的词向量如Google和Facebook。如果应用不是特定于某个领域的，那么可以使用预训练模型。\n",
    "\n",
    "skip-gram中，通过一个词预测其临近的词，skip-gram是包含了gap的n-grams。它对应的NN是两层的，隐藏层包含n个神经元（n是词向量维度），输入层和输出层都包含M个神经元（M是词汇表大小）。激活函数是softmax。\n",
    "\n",
    "softmax是通常用于分类问题的激活函数，softmax将输出限定在0到1之间，且其和为1，因此结果可视为“概率”。\n",
    "\n",
    "以下为使用fasttext的预训练模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format('/Users/andersc/data/word2vecs/wiki-news-300d-1M.vec')\n",
    "\n",
    "# 寻找不相关词\n",
    "model.doesnt_match('potato milk computer'.split())\n",
    "\n",
    "# 组合几个词\n",
    "model.most_similar(positive=['cooking', 'patatoes'], topn=5)\n",
    "\n",
    "# init magic\n",
    "model.most_similar(positive=['king', 'woman'], negative=['man'], topn=5)\n",
    "\n",
    "# [('queen', 0.7515910863876343),\n",
    "#  ('monarch', 0.6741327047348022),\n",
    "#  ('princess', 0.6713887453079224),\n",
    "#  ('kings', 0.6698989868164062),\n",
    "#  ('kingdom', 0.5971318483352661)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.4 训练自己的词向量表示\n",
    "\n",
    "依然用到gensim。\n",
    "\n",
    "* 预处理：句子集合；每个句子是token列表；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7、词就其位（CNN）\n",
    "\n",
    "* NLP中使用NN\n",
    "* 寻找词模式中的语义\n",
    "* 创建一个CNN\n",
    "* 将文本向量化以适用于NN\n",
    "* 训练一个CNN\n",
    "* 情感分析\n",
    "\n",
    "语义不仅仅蕴含在词本身，也包含在词之间的空隙里，词的顺序以及组合。词之间的连接使得深度、信息和复杂性成为可能。对人来说，仅靠词或n-grams，也不能了解完整的语义，所谓“只言片语”。除了日常对话，伟大的作者通过特有的方式表达思想，这些方式也是“模式”。机器对这些模式一无所知，除非有人告诉它们。\n",
    "\n",
    "NN发展极快，感知机之后是CNN和RNN之类。在Word2vec那儿可以看到，NN给NLP带来了全新的方法与视角。尽管NN的本初目的是让机器*学习*如何量化输入，但这个领域已经迅速从基本的分类、回归问题延伸至：翻译、chatbot、以某位作者的风格“写作”。\n",
    "\n",
    "## 7.1 Learning meaning\n",
    "\n",
    "一个词的本质和**它与其它词的关系**相关联，即看到一个句子或段落，它的整体意思，远超过通过字典把词一个个查出来所了解到的信息。关系至少有两种：\n",
    "\n",
    "* Word order：词序改变，意思随之改变\n",
    "\n",
    "The dog chased the cat.\n",
    "The cat chased the dog.\n",
    "\n",
    "* Word proximity（就近性？）：\n",
    "\n",
    "The ship's hull, despite years at sea, millions of tons of cargo, and two mid-sea collisions, shone like new. (hull与shone的关系）\n",
    "\n",
    "## 7.4 窄窗\n",
    "\n",
    "CNN先是用于图像处理，但也可以通过词向量用于NLP上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle padding input\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "\n",
    "def preprocess_data(filepath):\n",
    "    pos_label = 1\n",
    "    neg_label = 0\n",
    "    dataset = []\n",
    "    \n",
    "    pos_path = os.path.join(filepath, 'pos')\n",
    "    neg_path = os.path.join(filepath, 'neg')\n",
    "    paths = [(pos_path, pos_label), (neg_path, neg_label)]\n",
    "    for p, label in paths:\n",
    "        for fn in glob.glob(os.path.join(p, '*.txt')):\n",
    "            with open(fn, 'r') as f:\n",
    "                dataset.append((label, f.read()))\n",
    "                \n",
    "    shuffle(dataset)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = preprocess_data('data/aclImdb/train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " \"Xizao is a rare little movie. It is simple and undemanding, and at the same time so rewarding in emotion and joy. The story is simple, and the theme of old and new clashing is wonderfully introduced in the first scenes. This theme is the essence of the movie, but it would have fallen flat if it wasn't for the magnificent characters and the actors portraying them.<br /><br />The aging patriarch, Master Liu, is a relic of China's pre-expansion days. He runs a bath house in an old neighbourhood. Every single scene set in the bath house is a source of jelaousy for us stressed out, unhappy people. Not even hardened cynics can find any flaws in this wonderful setting.<br /><br />Master Liu's mentally handicapped son Er Ming is the second truly powerful character in the movie, coupled with his modern-life brother. The interactions between these three people, and the various visitors to the bath house, are amazingly detailed and heart-felt, with some scenes packing so much emotion it's beyond almost everything seen in movies.<br /><br />With its regime-critical message, this movie was not only censored, but also given unreasonably small coverage. It could be a coincidence, but when a movie of this caliber is virtually impossible to find, even on the internet(!), you can't help getting suspicious.<br /><br />So help free speech and the movie world, buy, rent, copy this wonderful movie, and if you happen to own the DVD, if there even is one, then share share share!\")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vecs = KeyedVectors.load_word2vec_format('/Users/andersc/data/word2vecs/GoogleNews-vectors-negative300.bin.gz', \n",
    "                                              limit=200000,\n",
    "                                              binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_vectorize(dataset):\n",
    "    vectorized = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        tokens = tokenizer.tokenize(sample[1])\n",
    "        sample_vecs = []\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                sample_vecs.append(word_vecs[token])\n",
    "            except KeyError:\n",
    "                pass\n",
    "        vectorized.append(sample_vecs)\n",
    "    return vectorized\n",
    "\n",
    "\n",
    "def collect_expected(dataset):\n",
    "    expected = [sample[0] for sample in dataset]\n",
    "    return expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_data = tokenize_and_vectorize(train_data)\n",
    "expected = collect_expected(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_point = int(len(vectorized_data) * 0.8)\n",
    "\n",
    "x_train = vectorized_data[:split_point]\n",
    "y_train = expected[:split_point]\n",
    "x_test = vectorized_data[split_point:]\n",
    "y_test = expected[split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 400 # maxlen of sentence\n",
    "batch_size = 32 \n",
    "embedding_dims = 300\n",
    "filters = 250\n",
    "kernel_size = 3 # like window size\n",
    "hidden_dims = 250\n",
    "epochs = 2\n",
    "\n",
    "\n",
    "def pad_trunc(data, maxlen):\n",
    "    new_data = []\n",
    "    zero_vector = []\n",
    "    for _ in range(len(data[0][0])):\n",
    "        zero_vector.append(0.0)\n",
    "    \n",
    "    for sample in data:\n",
    "        if len(sample) >= maxlen:\n",
    "            temp = sample[:maxlen]\n",
    "        else:\n",
    "            temp = sample\n",
    "            additional_elems = maxlen - len(sample)\n",
    "            for _ in range(additional_elems):\n",
    "                temp.append(zero_vector)\n",
    "        new_data.append(temp)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pad_trunc(x_train, max_len)\n",
    "x_test = pad_trunc(x_test, max_len)\n",
    "\n",
    "x_train = np.reshape(x_train, (len(x_train), max_len, embedding_dims))\n",
    "y_trainstain = np.array(y_train)\n",
    "x_test = np.reshape(x_test, (len(x_test), max_len, embedding_dims))\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Training...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 270s 14ms/step - loss: 0.3910 - acc: 0.8170 - val_loss: 0.3165 - val_acc: 0.8664\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 272s 14ms/step - loss: 0.2296 - acc: 0.9079 - val_loss: 0.3521 - val_acc: 0.8556\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a555ac940>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(\n",
    "    filters,\n",
    "    kernel_size,\n",
    "    padding='valid',\n",
    "    activation='relu',\n",
    "    strides=1,\n",
    "    input_shape=(max_len, embedding_dims)))\n",
    "\n",
    "# max pooling;\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# vanilla hidden layer;\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# project onto a single unit output layer, and squash it with a sigmoid\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Training...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "model_struct = model.to_json()\n",
    "json_dump(model_struct, 'cnn_model.json')\n",
    "\n",
    "model.save_weights('cnn_weights.h5')\n",
    "print('Model saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import model_from_json\n",
    "# with open(\"cnn_model.json\", \"r\") as json_file:\n",
    "#     json_string = json_file.read()\n",
    "# model = model_from_json(json_string)\n",
    "\n",
    "# model.load_weights('cnn_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03859694],\n",
       "       [0.99361515]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_1 = \"I'm hate that the dismal weather that had me down for so long, when will it break! Ugh, when does happiness return?  The sun is blinding and the puffy clouds are too thin.  I can't wait for the weekend.\"\n",
    "sample_2 = \"I really love that place, the warm weather and amazing beach.\"\n",
    "tests = [sample_1, sample_2]\n",
    "\n",
    "sample_vec = tokenize_and_vectorize([(-1, sample) for sample in tests])\n",
    "test_vec_list = pad_trunc(sample_vec, max_len)\n",
    "test_vec = np.reshape(test_vec_list, (len(test_vec_list), max_len, embedding_dims))\n",
    "model.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1]], dtype=int32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
